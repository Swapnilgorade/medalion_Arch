from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
import logging
import pytz
from datetime import datetime
import os


# CustomLogger class (unchanged, as above)
class CustomLogger:
    def __init__(self, log_directory="/tmp/", environment="dev"):
        self.log_directory = log_directory
        self.environment = environment.lower()
        self.logger = None
        self._setup_file_logger()
        self._setup_console_logger()

    def _get_log_file_path(self):
        current_dt = datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y%m%d_%H%M%S')
        log_filename = f"{self.environment}_cloudpandith_{current_dt}.log"
        return os.path.join(self.log_directory, log_filename)

    def _setup_file_logger(self):
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)

        final_path = self._get_log_file_path()
        log_level = logging.DEBUG if self.environment == "dev" else logging.WARNING

        logging.basicConfig(
            filename=final_path,
            level=log_level,
            format='%(asctime)s:%(levelname)s:%(message)s',
            datefmt='%d/%m/%Y %I:%M:%S %p',
            filemode='a'
        )

        logging.warning('File logger initialized for environment: %s', self.environment)

    def _setup_console_logger(self):
        self.logger = logging.getLogger('demologger')
        self.logger.setLevel(logging.DEBUG if self.environment == "dev" else logging.INFO)

        console_handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s: %(message)s',
            datefmt='%m/%d/%Y %I:%M:%S %p'
        )
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

        self.logger.info('Console logger initialized for environment: %s', self.environment)

    def get_logger(self):
        return self.logger


# Sample PySpark code with Delta table creation
if __name__ == "__main__":
    # Initialize CustomLogger
    log_directory = "/tmp/logs/"
    environment = "dev"
    custom_logger = CustomLogger(log_directory=log_directory, environment=environment)
    logger = custom_logger.get_logger()

    # Start SparkSession
    logger.info("Starting SparkSession...")
    spark = SparkSession.builder \
        .appName("PySparkDeltaExample") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()

    logger.info("SparkSession started successfully.")

    try:
        # Create a sample DataFrame
        logger.info("Creating sample DataFrame...")
        data = [("John", 28), ("Doe", 35), ("Jane", 25), ("Smith", 40)]
        columns = ["Name", "Age"]
        df = spark.createDataFrame(data, columns)

        # Perform a transformation
        logger.info("Filtering DataFrame for ages greater than 30...")
        filtered_df = df.filter(col("Age") > 30)

        # Write filtered DataFrame to Delta table
        delta_path = os.path.join(log_directory, "delta_table")
        logger.info(f"Writing filtered data to Delta table at path: {delta_path}")
        filtered_df.write.format("delta").mode("overwrite").save(delta_path)

        logger.info("Filtered data successfully written to Delta table.")

        # Read Delta table back and show contents
        logger.info(f"Reading Delta table from path: {delta_path}")
        delta_df = spark.read.format("delta").load(delta_path)
        delta_df.show()

    except Exception as e:
        logger.error(f"An error occurred: {str(e)}", exc_info=True)

    finally:
        logger.info("Stopping SparkSession...")
        spark.stop()
        logger.info("SparkSession stopped successfully.")
